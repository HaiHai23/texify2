{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\ASUS\\\\Documents\\\\GitHub\\\\texify'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded texify model to cpu with torch.float32 dtype\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from texify.inference import batch_inference\n",
    "from texify.model.model import load_model\n",
    "from texify.model.processor import load_processor\n",
    "from PIL import Image\n",
    "\n",
    "model = load_model()\n",
    "processor = load_processor()\n",
    "img = Image.open(\"C:\\\\Users\\\\ASUS\\\\Downloads\\\\test.jpg\") # Your image name here\n",
    "results = batch_inference([img], model, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\n\\\\begin{tabular}{l l}\\n\\\\begin{tabular}{l} \\\\begin{tabular}{l} \\\\begin{tabular}{l} \\\\begin{tabular}{l} \\\\begin{tabular} \\\\begin{tabular}{l} \\\\begin{tabular} \\\\begin{tabular}{l} \\\\begin{tabular} \\\\begin{tabular}{l} \\\\begin{tabular} \\\\begin{tabular}{l} \\\\begin{tabular} \\\\begin{tabular}{l} \\\\begin{tabular} \\\\begin{tabular}{l} \\\\begin{tabular} \\\\begin{tabular}{l} \\\\begin{tabular} \\\\begin{tabular}{l} \\\\begin{tabular} \\\\begin{tabular}{l} \\\\begin{tabular} \\\\begin{tabular}{l} \\\\begin{tabular}{l} \\\\begin{tabular}{l} \\\\begin{tabular}{l} \\\\begin{tabular}{l} \\\\begin{tabular}{l} \\\\begin{tabular}{l} \\\\begin{tabular}{l} \\\\begin{tabular}{l} \\\\begin{tabular}{l} \\\\begin{tabular}{l} \\\\begin{tabular}{l} \\\\begin{tabular}{l} \\\\begin{tabular}{l} \\\\begin{tabular}{l} \\\\begin{tabular}{l} \\\\begin{tabular}{l} ']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\\begin{tabular}{c c} \\hline \\multicolumn{2}{c}{UNDIUY(EN IIOAI)D(C & K$\\dot{\\gamma}$THI ICTION IOC SINH GIOI L$\\dot{\\gamma}$P 9}\\\\ \\hline P$\\dot{\\varepsilon}$chinn h$\\dot{\\alpha}$t & N$\\dot{\\alpha}$t 100: An. N$\\dot{\\alpha}$m h$\\dot{\\alpha}$ 2024-2025 \\\\ \\hline \\multicolumn{2}{c}{N$\\dot{\\alpha}$t 100: An. N$\\dot{\\alpha}$t 2024-2024} \\\\ \\hline \\multicolumn{2}{c}{N$\\dot{\\alpha}$t 100: An. N$\\dot{\\alpha}$t 2024-2025} \\\\ \\hline \\multicolumn{2}{c}{B$\\dot{\\alpha}$t 100: $\\dot{\\alpha}$t 2024-2025} \\\\ \\hline \\multicolumn{2}{c}{B$\\dot{\\alpha}$t 100: $\\dot{\\alpha}$t 2024-2025} \\\\ \\hline \\multicolumn{2\n"
     ]
    }
   ],
   "source": [
    "print(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "\\begin{tabular}{c c} \\hline \\multicolumn{2}{c}{UNDIUY(EN IIOAI)D(C & K$\\dot{\\gamma}$THI ICTION IOC SINH GIOI L$\\dot{\\gamma}$P 9}\\\\ \\hline P$\\dot{\\varepsilon}$chinn h$\\dot{\\alpha}$t & N$\\dot{\\alpha}$t 100: An. N$\\dot{\\alpha}$m h$\\dot{\\alpha}$ 2024-2025 \\\\ \\hline \\multicolumn{2}{c}{N$\\dot{\\alpha}$t 100: An. N$\\dot{\\alpha}$t 2024-2024} \\\\ \\hline \\multicolumn{2}{c}{N$\\dot{\\alpha}$t 100: An. N$\\dot{\\alpha}$t 2024-2025} \\\\ \\hline \\multicolumn{2}{c}{B$\\dot{\\alpha}$t 100: $\\dot{\\alpha}$t 2024-2025} \\\\ \\hline \\multicolumn{2}{c}{B$\\dot{\\alpha}$t 100: $\\dot{\\alpha}$t 2024-2025} \\\\ \\hline \\multicolumn{2"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "display(Markdown(results[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\\begin{tabular}{c c} \\hline \\multicolumn{2}{c}{UNDIUY(EN IIOAI)D(C & K$\\dot{\\gamma}$THI ICTION IOC SINH GIOI L$\\dot{\\gamma}$P 9}\\\\ \\hline P$\\dot{\\varepsilon}$chinn h$\\dot{\\alpha}$t & N$\\dot{\\alpha}$t 100: An. N$\\dot{\\alpha}$m h$\\dot{\\alpha}$ 2024-2025 \\\\ \\hline \\multicolumn{2}{c}{N$\\dot{\\alpha}$t 100: An. N$\\dot{\\alpha}$t 2024-2024} \\\\ \\hline \\multicolumn{2}{c}{N$\\dot{\\alpha}$t 100: An. N$\\dot{\\alpha}$t 2024-2025} \\\\ \\hline \\multicolumn{2}{c}{B$\\dot{\\alpha}$t 100: $\\dot{\\alpha}$t 2024-2025} \\\\ \\hline \\multicolumn{2}{c}{B$\\dot{\\alpha}$t 100: $\\dot{\\alpha}$t 2024-2025} \\\\ \\hline \\multicolumn{2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion completed. JSON files saved in: C:\\Users\\ASUS\\Downloads\\New folder\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "# Path to the folder with PNG and TXT files\n",
    "input_folder = 'C:\\\\Users\\\\ASUS\\\\Downloads\\\\New folder'\n",
    "output_folder = 'C:\\\\Users\\\\ASUS\\\\Downloads\\\\New folder'\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Function to convert PNG file to JSON (metadata only)\n",
    "def png_to_json(file_path, output_folder):\n",
    "    with Image.open(file_path) as img:\n",
    "        img_data = {\n",
    "            \"filename\": os.path.basename(file_path),\n",
    "            \"size\": img.size,\n",
    "            \"mode\": img.mode,\n",
    "            \"format\": img.format,\n",
    "        }\n",
    "    json_path = os.path.join(output_folder, os.path.basename(file_path).replace('.png', '.json'))\n",
    "    with open(json_path, 'w', encoding=\"utf-8\") as f:\n",
    "        json.dump(img_data, f)\n",
    "\n",
    "# Function to convert TXT file to JSON\n",
    "def txt_to_json(file_path, output_folder):\n",
    "    with open(file_path, 'r', encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "    json_data = {\n",
    "        \"filename\": os.path.basename(file_path),\n",
    "        \"content\": content\n",
    "    }\n",
    "    json_path = os.path.join(output_folder, os.path.basename(file_path).replace('.txt', '.json'))\n",
    "    with open(json_path, 'w', encoding=\"utf-8\") as f:\n",
    "        json.dump(json_data, f)\n",
    "\n",
    "# Process each file in the input folder\n",
    "for filename in os.listdir(input_folder):\n",
    "    file_path = os.path.join(input_folder, filename)\n",
    "    if filename.lower().endswith('.png'):\n",
    "        png_to_json(file_path, output_folder)\n",
    "    elif filename.lower().endswith('.txt'):\n",
    "        txt_to_json(file_path, output_folder)\n",
    "\n",
    "print(\"Conversion completed. JSON files saved in:\", output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, VisionEncoderDecoderModel, GenerationMixin, PretrainedConfig, PreTrainedModel, VisionEncoderDecoderConfig, AutoModelForCausalLM\n",
    "from transformers.models.donut.modeling_donut_swin import DonutSwinPatchEmbeddings, DonutSwinEmbeddings, DonutSwinModel, \\\n",
    "    DonutSwinEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion completed. JSON file saved as: C:\\Users\\ASUS\\Downloads\\New folder\\combined_data.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "# Path to the folder with PNG and TXT files\n",
    "input_folder = 'C:\\\\Users\\\\ASUS\\\\Downloads\\\\New folder'\n",
    "output_json_path = os.path.join(input_folder, 'combined_data.json')\n",
    "\n",
    "# Function to extract PNG file metadata\n",
    "def png_to_data(file_path):\n",
    "    with Image.open(file_path) as img:\n",
    "        img_data = {\n",
    "            \"filename\": os.path.basename(file_path),\n",
    "            \"size\": img.size,\n",
    "            \"mode\": img.mode,\n",
    "            \"format\": img.format,\n",
    "        }\n",
    "    return img_data\n",
    "\n",
    "# Function to extract TXT file content\n",
    "def txt_to_data(file_path):\n",
    "    with open(file_path, 'r', encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "    return {\n",
    "        \"filename\": os.path.basename(file_path),\n",
    "        \"content\": content\n",
    "    }\n",
    "\n",
    "# Collect data for all files\n",
    "combined_data = {}\n",
    "\n",
    "# Process each file in the input folder\n",
    "for filename in os.listdir(input_folder):\n",
    "    file_path = os.path.join(input_folder, filename)\n",
    "    if filename.lower().endswith('.png'):\n",
    "        combined_data[filename] = png_to_data(file_path)\n",
    "    elif filename.lower().endswith('.txt'):\n",
    "        combined_data[filename] = txt_to_data(file_path)\n",
    "\n",
    "# Save combined data to a single JSON file\n",
    "with open(output_json_path, 'w', encoding=\"utf-8\") as f:\n",
    "    json.dump(combined_data, f, indent=4)\n",
    "\n",
    "print(\"Conversion completed. JSON file saved as:\", output_json_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import os\n",
    "import sys\n",
    "os.chdir('C:\\\\Users\\\\ASUS\\\\Downloads\\\\New folder')\n",
    "\n",
    "files = os.listdir()\n",
    "imgs = [f for f in files if f.endswith('.png')]\n",
    "txts = [f for f in files if f.endswith('.txt')]\n",
    "\n",
    "def image_to_base64(image_path):\n",
    "    # Open the image file\n",
    "    with Image.open(image_path) as image:\n",
    "        # Create a BytesIO buffer to hold the image data\n",
    "        buffered = BytesIO()\n",
    "        # Save the image to the buffer in PNG format\n",
    "        image.save(buffered, format=\"PNG\")\n",
    "        # Get the base64 encoded string from the buffer\n",
    "        base64_string = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "    return base64_string\n",
    "\n",
    "def read_txt(file):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "out = [{'name':image_to_base64(img)} for img in imgs]\n",
    "for i in range(len(out)):\n",
    "    out[i]['equation'] = read_txt(txts[i])\n",
    "\n",
    "out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def list_of_dicts_to_jsonl(list_of_dicts, output_file):\n",
    "    with open(output_file, 'w') as f:\n",
    "        for item in list_of_dicts:\n",
    "            # Convert the dictionary to a JSON string and write it to the file\n",
    "            json_line = json.dumps(item)\n",
    "            f.write(json_line + '\\n')\n",
    "\n",
    "def list_of_dicts_to_json(list_of_dicts, output_file):\n",
    "    with open(output_file, 'w') as f:\n",
    "        # Convert the list of dictionaries to JSON and write it to the file\n",
    "        json.dump(list_of_dicts, f, indent=4)  # 'indent=4' makes it more readable\n",
    "\n",
    "list_of_dicts_to_json(out, 'data.json')\n",
    "list_of_dicts_to_jsonl(out, 'data.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.donut.modeling_donut_swin.DonutSwinModel'> is overwritten by shared encoder config: DonutSwinConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"depths\": [\n",
      "    2,\n",
      "    2,\n",
      "    14,\n",
      "    2\n",
      "  ],\n",
      "  \"drop_path_rate\": 0.1,\n",
      "  \"embed_dim\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"image_size\": [\n",
      "    420,\n",
      "    420\n",
      "  ],\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"mlp_ratio\": 4.0,\n",
      "  \"model_type\": \"donut-swin\",\n",
      "  \"num_channels\": 3,\n",
      "  \"num_heads\": [\n",
      "    4,\n",
      "    8,\n",
      "    16,\n",
      "    32\n",
      "  ],\n",
      "  \"num_layers\": 4,\n",
      "  \"patch_size\": 4,\n",
      "  \"path_norm\": true,\n",
      "  \"qkv_bias\": true,\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_2d_embeddings\": false,\n",
      "  \"use_absolute_embeddings\": false,\n",
      "  \"window_size\": 5\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.mbart.modeling_mbart.MBartForCausalLM'> is overwritten by shared decoder config: MBartConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 8,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"max_position_embeddings\": 1536,\n",
      "  \"model_type\": \"mbart\",\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 103, 2], [0, 107, 2], [0, 18191, 2], [0, 243, 2], [0, 103, 2], [0, 107, 2], [0, 28985, 2], [0, 243, 2], [0, 93, 2], [0, 95, 2], [0, 4123, 2], [0, 34, 2], [0, 243, 2], [0, 94, 2], [0, 26215, 117, 2], [0, 100, 2], [0, 94, 2], [0, 243, 2], [0, 102, 2], [0, 94, 2], [0, 16787, 2], [0, 89, 2], [0, 243, 2], [0, 26215, 121, 2], [0, 111, 2], [0, 243, 2], [0, 98, 2], [0, 101, 2], [0, 100, 2], [0, 93, 2], [0, 243, 2], [0, 98, 2], [0, 87, 2], [0, 100, 2], [0, 94, 2], [0, 243, 2], [0, 95, 2], [0, 100, 2], [0, 243, 2], [0, 106, 2], [0, 104, 2], [0, 101, 2], [0, 100, 2], [0, 93, 2], [0, 243, 2], [0, 99, 2], [0, 26215, 130, 2], [0, 106, 2], [0, 243, 2], [0, 99, 2], [0, 26215, 140, 2], [0, 36, 2]]\n",
      "[[0, 73, 2], [0, 87, 2], [0, 107, 2], [0, 243, 2], [0, 99, 2], [0, 28224, 268, 2], [0, 95, 2], [0, 243, 2], [0, 99, 2], [0, 150, 140, 2], [0, 87, 2], [0, 243, 2], [0, 106, 2], [0, 94, 2], [0, 95, 2], [0, 243, 2], [0, 151, 262, 2], [0, 26215, 117, 2], [0, 95, 2], [0, 243, 2], [0, 94, 2], [0, 28224, 258, 2], [0, 89, 2], [0, 34, 2], [0, 243, 2], [0, 89, 2], [0, 5458, 2], [0, 243, 2], [0, 88, 2], [0, 87, 2], [0, 101, 2], [0, 243, 2], [0, 24, 2], [0, 243, 2], [0, 105, 2], [0, 151, 125, 2], [0, 243, 2], [0, 106, 2], [0, 28224, 278, 2], [0, 243, 2], [0, 24, 2], [0, 243, 2], [0, 88, 2], [0, 107, 2], [0, 28224, 264, 2], [0, 100, 2], [0, 243, 2], [0, 104, 2], [0, 26215, 123, 2], [0, 107, 2], [0, 243, 2], [0, 97, 2], [0, 94, 2], [0, 95, 2], [0, 243, 2], [0, 88, 2], [0, 95, 2], [0, 26215, 146, 2], [0, 106, 2], [0, 243, 2], [0, 99, 2], [0, 46248, 2], [0, 100, 2], [0, 94, 2], [0, 243, 2], [0, 106, 2], [0, 104, 2], [0, 28224, 276, 2], [0, 243, 2], [0, 106, 2], [0, 94, 2], [0, 18191, 2], [0, 100, 2], [0, 94, 2], [0, 243, 2], [0, 24, 2], [0, 243, 2], [0, 106, 2], [0, 28224, 278, 2]]\n",
      "[[0, 57, 2], [0, 107, 2], [0, 28224, 270, 2], [0, 89, 2], [0, 243, 2], [0, 105, 2], [0, 28224, 262, 2], [0, 100, 2], [0, 93, 2], [0, 243, 2], [0, 105, 2], [0, 26215, 144, 2], [0, 243, 2], [0, 89, 2], [0, 94, 2], [0, 28224, 254, 2], [0, 243, 2], [0, 89, 2], [0, 94, 2], [0, 101, 2], [0, 243, 2], [0, 88, 2], [0, 26215, 117, 2], [0, 100, 2], [0, 243, 2], [0, 89, 2], [0, 5458, 2], [0, 243, 2], [0, 100, 2], [0, 94, 2], [0, 28224, 130, 2], [0, 100, 2], [0, 93, 2], [0, 243, 2], [0, 100, 2], [0, 93, 2], [0, 153, 131, 2], [0, 28224, 274, 2], [0, 95, 2], [0, 243, 2], [0, 151, 262, 2], [0, 26215, 117, 2], [0, 106, 2], [0, 243, 2], [0, 151, 262, 2], [0, 153, 131, 2], [0, 28224, 119, 2], [0, 89, 2], [0, 243, 2], [0, 106, 2], [0, 94, 2], [0, 18191, 2], [0, 100, 2], [0, 94, 2], [0, 243, 2], [0, 89, 2], [0, 18692, 2], [0, 100, 2], [0, 93, 2], [0, 243, 2], [0, 106, 2], [0, 94, 2], [0, 91, 2], [0, 101, 2], [0, 243, 2], [0, 99, 2], [0, 28224, 270, 2], [0, 106, 2]]\n",
      "[[0, 99, 2], [0, 28224, 258, 2], [0, 95, 2], [0, 243, 2], [0, 100, 2], [0, 93, 2], [0, 153, 131, 2], [0, 28224, 274, 2], [0, 95, 2], [0, 243, 2], [0, 100, 2], [0, 28224, 248, 2], [0, 243, 2], [0, 102, 2], [0, 94, 2], [0, 28224, 121, 2], [0, 89, 2], [0, 53, 2], [0, 243, 2], [0, 76, 2], [0, 26215, 278, 2], [0, 111, 2], [0, 243, 2], [0, 106, 2], [0, 94, 2], [0, 46248, 2], [0, 243, 2], [0, 88, 2], [0, 26215, 117, 2], [0, 100, 2], [0, 243, 2], [0, 94, 2], [0, 10995, 2], [0, 111, 2], [0, 243, 2], [0, 90, 2], [0, 18191, 2], [0, 100, 2], [0, 94, 2], [0, 243, 2], [0, 89, 2], [0, 94, 2], [0, 16787, 2], [0, 106, 2], [0, 243, 2], [0, 106, 2], [0, 94, 2], [0, 28224, 274, 2], [0, 95, 2], [0, 243, 2], [0, 93, 2], [0, 95, 2], [0, 87, 2], [0, 100, 2], [0, 243, 2], [0, 151, 262, 2], [0, 28224, 248, 2], [0, 243, 2], [0, 98, 2], [0, 26215, 138, 2], [0, 100, 2], [0, 93, 2], [0, 243, 2], [0, 99, 2], [0, 46248, 2], [0, 100, 2], [0, 94, 2], [0, 243, 2], [0, 105, 2], [0, 107, 2], [0, 111, 2], [0, 243, 2], [0, 100, 2], [0, 93, 2], [0, 26215, 127, 2], [0, 99, 2], [0, 36, 2]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4/4 [00:00<00:00, 15.42 examples/s]\u001b[A\u001b[A\n",
      "\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 21.81 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 56, 2], [0, 95, 2], [0, 28224, 248, 2], [0, 100, 2], [0, 243, 2], [0, 89, 2], [0, 5458, 2], [0, 243, 2], [0, 108, 2], [0, 150, 140, 2], [0, 100, 2], [0, 93, 2], [0, 243, 2], [0, 100, 2], [0, 28224, 270, 2], [0, 95, 2], [0, 243, 2], [0, 106, 2], [0, 94, 2], [0, 28224, 123, 2], [0, 111, 2], [0, 34, 2], [0, 243, 2], [0, 98, 2], [0, 10995, 2], [0, 100, 2], [0, 94, 2], [0, 243, 2], [0, 94, 2], [0, 26215, 119, 2], [0, 95, 2], [0, 34, 2], [0, 243, 2], [0, 108, 2], [0, 150, 140, 2], [0, 100, 2], [0, 93, 2], [0, 243, 2], [0, 151, 262, 2], [0, 26215, 138, 2], [0, 89, 2], [0, 243, 2], [0, 103, 2], [0, 107, 2], [0, 111, 2], [0, 28224, 246, 2], [0, 100, 2], [0, 243, 2], [0, 97, 2], [0, 95, 2], [0, 100, 2], [0, 94, 2], [0, 243, 2], [0, 106, 2], [0, 26215, 146, 2], [0, 243, 2], [0, 108, 2], [0, 18191, 2], [0, 243, 2], [0, 106, 2], [0, 94, 2], [0, 28224, 246, 2], [0, 99, 2], [0, 243, 2], [0, 98, 2], [0, 28224, 121, 2], [0, 89, 2], [0, 243, 2], [0, 151, 262, 2], [0, 28224, 256, 2], [0, 87, 2], [0, 243, 2], [0, 108, 2], [0, 28224, 272, 2], [0, 95, 2], [0, 243, 2], [0, 90, 2], [0, 95, 2], [0, 28224, 252, 2], [0, 100, 2]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [17:59<?, ?it/s]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 63 at dim 3 (got 56)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 108\u001b[0m\n\u001b[0;32m    100\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m    101\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    102\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m    103\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[0;32m    104\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39meval_dataset,\n\u001b[0;32m    105\u001b[0m )\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# Step 5: Fine-tune the model\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# Save the fine-tuned model, tokenizer, and feature extractor\u001b[39;00m\n\u001b[0;32m    111\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./fine_tuned_texify\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\trainer.py:2122\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2120\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2121\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2123\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2127\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\trainer.py:2426\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2424\u001b[0m update_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2425\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step \u001b[38;5;241m!=\u001b[39m (total_updates \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[1;32m-> 2426\u001b[0m batch_samples, num_items_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2427\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs \u001b[38;5;129;01min\u001b[39;00m batch_samples:\n\u001b[0;32m   2428\u001b[0m     step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\trainer.py:5038\u001b[0m, in \u001b[0;36mTrainer.get_batch_samples\u001b[1;34m(self, epoch_iterator, num_batches)\u001b[0m\n\u001b[0;32m   5036\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[0;32m   5037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 5038\u001b[0m         batch_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mnext\u001b[39m(epoch_iterator)]\n\u001b[0;32m   5039\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m   5040\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\accelerate\\data_loader.py:552\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 552\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dataloader_iter)\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\data\\data_collator.py:92\u001b[0m, in \u001b[0;36mdefault_data_collator\u001b[1;34m(features, return_tensors)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# In this function we'll make the assumption that all `features` in the batch\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# have the same attributes.\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# So we will look at the first element as a proxy for what attributes exist\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# on the whole batch.\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch_default_data_collator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf_default_data_collator(features)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\data\\data_collator.py:158\u001b[0m, in \u001b[0;36mtorch_default_data_collator\u001b[1;34m(features)\u001b[0m\n\u001b[0;32m    156\u001b[0m             batch[k] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39mstack([f[k] \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m features]))\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 158\u001b[0m             batch[k] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 63 at dim 3 (got 56)"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import io\n",
    "from PIL import Image\n",
    "from transformers import AutoTokenizer, VisionEncoderDecoderModel, Trainer, TrainingArguments, AutoFeatureExtractor\n",
    "from datasets import load_dataset, Dataset\n",
    "import torch\n",
    "import json\n",
    "\n",
    "# Step 1: Load and preprocess your JSON data\n",
    "# Load data from your JSON file\n",
    "#data = load_dataset(\"json\",data_files=\"C:\\\\Users\\\\ASUS\\\\Downloads\\\\New folder\\\\data.json\")\n",
    "    # Add more entries here\n",
    "with open(\"C:\\\\Users\\\\ASUS\\\\Downloads\\\\New folder\\\\data.json\", \"r\") as f:\n",
    "    data = json.load(f)  # Load JSON data as a list of dictionaries\n",
    "\n",
    "# Decode base64 image and convert to PIL\n",
    "def decode_image(image_base64):\n",
    "    decoded = base64.b64decode(image_base64)\n",
    "    return Image.open(io.BytesIO(decoded)).convert(\"RGB\")\n",
    "\n",
    "# Preprocess data to convert into a Dataset object\n",
    "def preprocess_data(data):\n",
    "    processed_data = {\"image\": [], \"text\": []}\n",
    "    for item in data:\n",
    "        if \"name\" not in item or \"equation\" not in item:\n",
    "            print(f\"Missing keys in item: {item}\")  # Debug print\n",
    "            continue  # Skip items that don't have the necessary keys\n",
    "        processed_data[\"image\"].append(decode_image(item[\"name\"]))\n",
    "        processed_data[\"text\"].append(item[\"equation\"])\n",
    "    return Dataset.from_dict(processed_data)\n",
    "\n",
    "# Create dataset\n",
    "dataset = preprocess_data(data)\n",
    "\n",
    "# Split dataset for training and validation\n",
    "train_test_split = dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "eval_dataset = train_test_split[\"test\"]\n",
    "\n",
    "# Step 2: Load model, tokenizer, and feature extractor\n",
    "model_name = \"vikp/texify\"  # Adjust with your model name if needed\n",
    "model = VisionEncoderDecoderModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#feature_extractor = model.encoder. # Extractor for images\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\n",
    "\n",
    "# Preprocess function for dataset\n",
    "def preprocess_function(examples):\n",
    "    max_length = 1000  # Set the desired maximum length for all sequences\n",
    "\n",
    "    # Preprocess images and resize to ensure consistent shape\n",
    "    pixel_values = feature_extractor(\n",
    "        images=examples[\"image\"], \n",
    "        return_tensors=\"pt\", \n",
    "    ).pixel_values\n",
    "\n",
    "    # Tokenize text without padding and truncation\n",
    "    tokenized_labels = [tokenizer(text)[\"input_ids\"] for text in examples[\"text\"]]\n",
    "    print(tokenized_labels)\n",
    "\n",
    "    # Manually pad or truncate each sequence to `max_length`\n",
    "    padded_labels = []\n",
    "    for label_seq in tokenized_labels:\n",
    "        # Truncate if longer than max_length\n",
    "        if len(label_seq) > max_length:\n",
    "            label_seq = label_seq[:max_length]\n",
    "        # Pad with the pad token ID if shorter than max_length\n",
    "        else:\n",
    "            label_seq = label_seq + [tokenizer.pad_token_id] * (max_length - len(label_seq))\n",
    "        \n",
    "        # Replace pad tokens with -100 for ignored positions in the loss calculation\n",
    "        label_seq = [token if token != tokenizer.pad_token_id else -100 for token in label_seq]\n",
    "        # print(label_seq)\n",
    "        padded_labels.append(label_seq)\n",
    "\n",
    "    # Convert to tensor\n",
    "    labels = torch.tensor(padded_labels)\n",
    "    # print(len(labels))\n",
    "\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "# Apply preprocessing\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=False)\n",
    "eval_dataset = eval_dataset.map(preprocess_function, batched=False)\n",
    "\n",
    "# Step 3: Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine_tuned_texify\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    # per_device_train_batch_size=2,\n",
    "    # per_device_eval_batch_size=2,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_steps=500,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "# Step 4: Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "# Step 5: Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model, tokenizer, and feature extractor\n",
    "trainer.save_model(\"./fine_tuned_texify\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_texify\")\n",
    "feature_extractor.save_pretrained(\"./fine_tuned_texify\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
